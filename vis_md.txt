import streamlit as st
from langchain_openai import ChatOpenAI
import os
import httpx
import base64
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
API_KEY = os.getenv("AZURE_API_KEY")

# Initialize HTTP client (skip SSL verification if needed)
client = httpx.Client(verify=False)

# Initialize the vision model
llm = ChatOpenAI(
    model="azure_ai/genailab-maas-Llama-3.2-90B-Vision-Instruct",  # or Phi-3.5 Vision
    temperature=0,
    base_url="https://genailab.tcs.in",
    api_key=API_KEY,
    http_client=client
)

# Streamlit UI
st.title("üñºÔ∏è Visual Question Answering with Azure Vision Models")

uploaded_file = st.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])
question = st.text_input("Ask a question about the image")

if uploaded_file and question:
    # Convert uploaded image to base64
    image_bytes = uploaded_file.read()
    image_base64 = base64.b64encode(image_bytes).decode("utf-8")
    image_url = f"data:image/jpeg;base64,{image_base64}"

    # Send text + image to the model
    response = llm.invoke([
        {
            "role": "user",
            "content": [
                {"type": "text", "text": question},
                {"type": "image_url", "image_url": {"url": image_url}}
            ]
        }
    ])

    # Display results
    st.image(uploaded_file, caption="Uploaded Image", use_column_width=True)
    st.subheader("Answer")
    st.write(response.content)
